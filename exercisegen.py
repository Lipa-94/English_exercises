import re
import random

import pandas as pd
import numpy as np
import gensim
import spacy
import pyinflect
import streamlit  as st

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import gensim.downloader as api

np.random.seed(123)
random.seed(123)

class ExerciseGen():
    
    def __init__(self):
        # –º–∞–ª–∞—è –º–æ–¥–µ–ª—å spacy
        self.__nlp = spacy.load("en_core_web_sm")

        # –º–∞–ª–∞—è –º–æ–¥–µ–ª—å glove wiki
        # –≤–Ω–∏–º–∞–Ω–∏–µ - –æ—á–µ–Ω—å –¥–æ–ª–≥–æ —Å–∫–∞—á–∏–≤–∞–µ—Ç, –µ—Å–ª–∏ –æ–Ω–∞ –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
        self.__model = api.load("glove-wiki-gigaword-100")
                
        np.random.seed(123)
        random.seed(123)
    
    # –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    def open_text(self, text):
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
        paragraphs = text.split("\n")  # Splitting text into paragraphs
        dataset = pd.DataFrame({'raw': paragraphs})
        dataset = dataset[dataset['raw'] != '']

        # –ü–µ—Ä–µ–Ω–æ—Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
        rows_list = []
        list_text = dataset['raw'].values
        for i in list_text:
            doc = self.__nlp(i)
            a = [sent.text.strip() for sent in doc.sents]
            for j in a:
                rows_list.append(j)
        df = pd.DataFrame(rows_list, columns=['raw'])

        return df
    
    # –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    def open_file(self, file):
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
        dataset = pd.read_csv(file, names=['raw'], delimiter="\t")

        # –ü–µ—Ä–µ–Ω–æ—Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
        rows_list = []
        list_text = dataset['raw'].values
        for i in list_text:
            doc = self.__nlp(i)
            a = [sent.text.strip() for sent in doc.sents]
            for j in a:
                rows_list.append(j)
        df = pd.DataFrame(rows_list, columns=['raw'])

        return df
    
    # –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–±–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é –∏–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é –∫–∞–≤—ã—á–∫—É –≤ —Ç–µ–∫—Å—Ç
    def quots_func(self, text):
        # –°—á–∏—Ç–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –≤ —Å—Ç—Ä–æ–∫–µ. –°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ, —Å–∫–æ–ª—å–∫–æ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö –∏ —Å–∫–æ–ª—å–∫–æ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö
        count_q = 0
        count_q_open = 0

        # –ë–µ—Ä–µ–º –∏–∑ –≤—Å–µ–π —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏ –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –∏ —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        for i in range(len(text)):
            if text[i] == '"':
                count_q += 1
                try:
                    if re.sub(r'[^a-zA-Z]', '', text[i+1]) != '':
                        count_q_open += 1 
                except:
                    pass

        # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö –∫–∞–≤—ã—á–µ–∫
        count_q_close = count_q - count_q_open

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É: –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö —Å—Ç—Ä–æ–∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
        if count_q_close == count_q_open:
            return text
        # –ï—Å–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö —Å–∫–æ–±–æ–∫ –±–æ–ª—å—à–µ, —á–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–≤—ã—á–∫—É –≤ –Ω–∞—á–∞–ª–æ
        elif count_q_close > count_q_open:
            return '"'+text
        # –ï—Å–ª–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö —Å–∫–æ–±–æ–∫ –±–æ–ª—å—à–µ, —á–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–≤—ã—á–∫—É –≤ –∫–æ–Ω–µ—Ü
        else:
            return text+'"'
        
    # –§—É–Ω–∫—Ü–∏—è, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—é—â–∞—è –¥–∞—Ç–∞—Å–µ—Ç: –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç—Å—è –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫, –∫–∞–≤—ã—á–∫–∏
    def beautify_text(self, df):
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ–º—Å—è —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã, –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –µ–µ –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å—Ç—Ä–æ–∫–µ, 
        # —Ç.–∫. —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —ç—Ç–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä—è–º–æ–π —Ä–µ—á–∏
        df['shift_raw'] = df['raw'].shift(periods=-1, fill_value='')
        df['raw_regex'] = df['raw'].str[:1].str.replace(r'[^a-z]', '', regex=True)
        df['shift_raw_regex'] = df['raw_regex'].shift(periods=-1, fill_value='')
        df.loc[df['shift_raw_regex'] != '', 'raw'] = (df.loc[df['shift_raw_regex'] != '', 'raw'] + 
                                                      '" ' + 
                                                      df.loc[df['shift_raw_regex'] != '', 'shift_raw'])
        df = df[df['raw_regex'] == '']
        df = df.drop(['shift_raw', 'raw_regex', 'shift_raw_regex'], axis=1)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–≤—ã—á–µ–∫
        df['raw'] = df['raw'].apply(self.quots_func)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º
        df = df.reset_index(drop=True)
        df = df.reset_index()
        df = df.rename(columns={"index": "row_num"})

        return df
    
    # –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ø—Ä–æ–ø—É—Å–∫ —Å–ª–æ–≤–∞, –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, —Å–∏–Ω–æ–Ω–∏–º, –∞–Ω—Ç–æ–Ω–∏–º
    # –ü—Ä–æ–±–ª–µ–º–∞: –∏–Ω–æ–≥–¥–∞ –≤—ã–≤–æ–¥–∏—Ç —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
    def select_word_syn_ant(self, text, pos=['NOUN', 'VERB', 'ADJ', 'ADV'], q_words=1):
        task_type = 'select_word_syn_ant'
        task_text = text
        task_object = []
        task_options = []
        task_answer = []
        task_result = []
        task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ'

        # –î–ª—è –∑–∞–¥–∞–Ω–∏—è –≤—ã–±–∏—Ä–∞—é—Ç—Å—è —Ä–∞–Ω–¥–æ–º–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ q_words
        for token in self.__nlp(text):
            if token.pos_ in pos:
                task_object.append(token.text)

        if len(task_object) > 1:
            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏ –ø—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º –∏—Ö –≤ —Ç–æ–º –ø–æ—Ä—è–¥–∫–µ, –≤ –∫–∞–∫–æ–º –æ–Ω–∏ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –∫ —Ç–µ–∫—Å—Ç–µ
            order = {number:task for number,task in enumerate(task_object)}
            task_object = random.sample(task_object, k=min(q_words, len(task_object)))
            order_list = []
            for i in task_object:
                order_list.append(list(order.keys())[list(order.values()).index(i)])
            order_list, task_object = zip(*sorted(zip(order_list, task_object)))
            task_object = list(task_object)

            for token in task_object:
                task_text = task_text.replace(token, '_____')
                task_answer.append(token)
                task_result.append('')

            task_options = [[token] for token in task_object]

            for token in task_options:
                token_new = token[0].lower()

                # –ò—â–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
                synonyms = self.__model.similar_by_word(token_new)
                synonyms = [ _[0] for _ in synonyms]
                synonyms = [_.text for _ in self.__nlp(' '.join(synonyms)) if not _.is_stop]
                try:
                    if synonyms[0] == token_new:
                        token.append(synonyms[1].title() if token[0].istitle() else synonyms[1])
                    else:
                        token.append(synonyms[0].title() if token[0].istitle() else synonyms[0])
                except:
                    pass

                # –ò—â–µ–º –∞–Ω—Ç–æ–Ω–∏–º—ã
                antonyms = self.__model.most_similar(positive=[token_new, 'bad'], negative=['good'])
                antonyms = [ _[0] for _ in antonyms]
                antonyms = [_.text for _ in self.__nlp(' '.join(antonyms)) if not _.is_stop]
                try:
                    if antonyms[0] == token_new or antonyms[0].title() == token[1] or antonyms[0] == token[1]:
                        token.append(antonyms[1].title() if token[0].istitle() else antonyms[1])
                    else:
                        token.append(antonyms[0].title() if token[0].istitle() else antonyms[0])
                except:
                    pass

                random.shuffle(token)
        else:
            task_object = np.nan
            task_options = np.nan
            task_answer = np.nan
            task_result = np.nan
            task_description = np.nan

        return {'raw' : text,
                'task_type' : task_type,
                'task_text' : task_text,
                'task_object' : task_object,
                'task_options' : task_options,
                'task_answer' : task_answer,
                'task_result' : task_result,
                'task_description' : task_description,
                'task_total': 0
                }
    
    # –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ø—Ä–æ–ø—É—Å–∫ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ, –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞: 3 —Ñ–æ—Ä–º—ã –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ
    def select_word_adv(self, text):
        task_text = text
        task_type = 'select_word_adv'
        task_object = []
        task_options = []
        task_answer = []
        task_result = []
        task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ'

        # –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å –ø–æ–º–æ—â—å—é pyinflect
        for token in self.__nlp(text):
            # –≤—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ, –Ω–æ —Ç–æ–ª—å–∫–æ —Ç–µ, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –≤—Å–µ 3 —Ñ–æ—Ä–º—ã
            if (token.pos_=='ADJ' and 
                token._.inflect('JJ') != None and 
                token._.inflect('JJR') != None and 
                token._.inflect('JJS') != None):
                task_text = task_text.replace(token.text, '_____')
                task_object.append(token.text)
                task_answer.append(token.text)
                task_result.append('')
                task_adv_options = []
                task_adv_options.append(token._.inflect('JJ'))  # JJ      Adjective
                task_adv_options.append(token._.inflect('JJR')) # JJR     Adjective, comparative
                task_adv_options.append(token._.inflect('JJS')) # JJS     Adjective, superlative
                task_options.append(task_adv_options)

        if task_object == []:
            task_object = np.nan
            task_options = np.nan
            task_answer = np.nan
            task_result = np.nan
            task_description = np.nan

        return {'raw' : text,
                'task_type' : task_type,
                'task_text' : task_text,
                'task_object' : task_object,
                'task_options' : task_options,
                'task_answer' : task_answer,
                'task_result' : task_result,
                'task_description' : task_description,
                'task_total': 0
                }
    
    # –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ø—Ä–æ–ø—É—Å–∫ –≥–ª–∞–≥–æ–ª–∞, –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞: 3 —Ñ–æ—Ä–º—ã –≥–ª–∞–≥–æ–ª–∞
    def select_word_verb(self, text):
        task_type = 'select_word_verb'
        task_text = text
        task_object = []
        task_options = []
        task_answer = []
        task_result = []
        task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –≥–ª–∞–≥–æ–ª–∞'

        # –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –≥–ª–∞–≥–æ–ª–∞ —Å –ø–æ–º–æ—â—å—é pyinflect
        for token in self.__nlp(text):
            if (token.pos_=='VERB'):
                task_text = task_text.replace(token.text, '_____')
                task_object.append(token.text)
                task_answer.append(token.text)
                task_result.append('')
                task_adv_options = []
                for i in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']:
                    # * VB      Verb, base form
                    # * VBD     Verb, past tense
                    # * VBG     Verb, gerund or present participle
                    # * VBN     Verb, past participle
                    # * VBP     Verb, non-3rd person singular present
                    # * VBZ     Verb, 3rd person singular present
                    # * MD      Modal'''
                    if token._.inflect(i) not in task_adv_options and token._.inflect(i) != None:
                        task_adv_options.append(token._.inflect(i)) 
                task_options.append(task_adv_options)

        if task_object == []:
            task_object = np.nan
            task_options = np.nan
            task_answer = np.nan
            task_result = np.nan
            task_description = np.nan

        return {'raw' : text,
                'task_type' : task_type,
                'task_text' : task_text,
                'task_object' : task_object,
                'task_options' : task_options,
                'task_answer' : task_answer,
                'task_result' : task_result,
                'task_description' : task_description,
                'task_total': 0
                }
    
    # –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã, –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞: –Ω–∞–∑–≤–∞–Ω–∏—è —á–∞—Å—Ç–µ–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    def select_memb_groups(self, text, q_words=1):
        task_type = 'select_memb_groups'
        task_text = text
        task_object = []
        task_options = []
        task_answer = []
        task_result = []
        task_description = '–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —á–µ–º —è–≤–ª—è–µ—Ç—Å—è –≤—ã–¥–µ–ª–µ–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞'

        # –î–ª—è –∑–∞–¥–∞–Ω–∏—è –≤—ã–±–∏—Ä–∞—é—Ç—Å—è —Ä–∞–Ω–¥–æ–º–Ω—ã–µ chunk –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ q_words
        for chunk in self.__nlp(text).noun_chunks:
            task_object.append(chunk)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∏ –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. –í —Ç–µ–∫—Å—Ç–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ chunk
        if len(task_object) > 1:
            order = {number:task for number,task in enumerate(task_object)}
            task_object = random.sample(task_object, k=min(q_words, len(task_object)))
            order_list = []
            for i in task_object:
                order_list.append(list(order.keys())[list(order.values()).index(i)])
            order_list, task_object = zip(*sorted(zip(order_list, task_object)))
            task_object = list(task_object)

            for chunk in task_object:
                task_text = task_text.replace(chunk.text, '**'+chunk.text+'**')
                task_answer.append(spacy.explain(chunk.root.dep_))
                task_object = chunk.text
                task_options.append('')
                task_result.append('')

            # –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏–∑ –≤—Å–µ—Ö –æ–ø–∏—Å–∞–Ω–∏–π —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –∏–∑ task_answer
            # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ 3-—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞, —Ç–æ –¥–æ–∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            unique_answers = list(set(task_answer))
            dep_list = ['clausal modifier of noun (adjectival clause)', 'adjectival complement', 'adverbial clause modifier', 
                    'adverbial modifier', 'agent', 'adjectival modifier', 'appositional modifier', 'attribute', 'auxiliary', 
                    'auxiliary (passive)', 'case marking', 'coordinating conjunction', 'clausal complement', 'compound', 
                    'conjunct', 'copula', 'clausal subject', 'clausal subject (passive)', 'dative', 'unclassified dependent', 
                    'determiner', 'direct object', 'expletive', 'interjection', 'marker', 'meta modifier', 'negation modifier', 
                    'noun compound modifier', 'noun phrase as adverbial modifier', 'nominal subject', 'nominal subject (passive)', 
                    'object predicate', 'object', 'oblique nominal', 'complement of preposition', 'object of preposition', 
                    'possession modifier', 'pre-correlative conjunction', 'prepositional modifier', 'particle', 'punctuation', 
                    'modifier of quantifier', 'relative clause modifier', 'root', 'open clausal complement']
            for i in unique_answers:
                try:
                    dep_list.remove(i)
                except:
                    pass
            if len(unique_answers) == 2:
                unique_answers.append(random.choice(dep_list))
            elif len(unique_answers) == 1:
                unique_answers.extend(random.sample(dep_list, k=2))
            random.shuffle(unique_answers)
            task_options = [unique_answers for _ in task_options]
        else:
            task_object = np.nan
            task_options = np.nan
            task_answer = np.nan
            task_result = np.nan
            task_description = np.nan

        return {'raw' : text,
                'task_type' : task_type,
                'task_text' : task_text,
                'task_object' : task_object,
                'task_options' : task_options,
                'task_answer' : task_answer,
                'task_result' : task_result,
                'task_description' : task_description,
                'task_total': 0
                }
    
    # –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä, –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞–º–∏ –≥–ª–∞–≥–æ–ª–∞
    def select_sent_verb(self, text):
        task_type = 'select_sent_verb'
        task_text = text
        task_object = [text]
        task_options = [text]
        task_answer = [text]
        task_result = ['']
        task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º–æ–π –≥–ª–∞–≥–æ–ª–∞'

        new_sent_1, new_sent_2 = text, text
        i=5
        count_verbs = 0
        for token in self.__nlp(text):
            if (token.pos_=='VERB'):
                # –°–æ—Å—Ç–∞–≤–ª—è–µ–º –ª–∏—Å—Ç —Ñ–æ—Ä–º —Å–ª–æ–≤, –Ω–µ –≤–∫–ª—é—á–∞—é—â–∏–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –≥–ª–∞–≥–æ–ª–∞
                verb_forms = []
                for i in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']:
                    # * VB      Verb, base form
                    # * VBD     Verb, past tense
                    # * VBG     Verb, gerund or present participle
                    # * VBN     Verb, past participle 
                    # * VBP     Verb, non-3rd person singular present
                    # * VBZ     Verb, 3rd person singular present
                    # * MD      Modal'''
                    if token._.inflect(i) != token.text and token._.inflect(i) != None and token._.inflect(i) not in verb_forms:
                        verb_forms.append(token._.inflect(i))

                new_word_1 = random.choice(verb_forms)
                new_word_1 = new_word_1.title() if token.text.istitle() else new_word_1
                new_sent_1 = new_sent_1.replace(token.text, new_word_1)
                verb_forms.remove(new_word_1)
                new_word_2 = random.choice(verb_forms)
                new_word_2 = new_word_1.title() if token.text.istitle() else new_word_2
                new_sent_2 = new_sent_2.replace(token.text, new_word_2)
                count_verbs += 1

        if count_verbs > 0 and len(text) < 100:
            task_options.append(new_sent_1)
            task_options.append(new_sent_2)
            random.shuffle(task_options)
        else:
            task_object = np.nan
            task_options = np.nan
            task_answer = np.nan
            task_result = np.nan
            task_description = np.nan

        return {'raw' : text,
                'task_type' : task_type,
                'task_text' : task_text,
                'task_object' : task_object,
                'task_options' : task_options,
                'task_answer' : task_answer,
                'task_result' : task_result,
                'task_description' : task_description,
                'task_total': 0
                }
    
    # –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä, –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å–ª–æ–≤–æ–º, —Å —Å–∏–Ω. –∏–ª–∏ –∞–Ω—Ç.
    def select_sent_word(self, text, pos=['NOUN', 'VERB', 'ADV', 'ADJ'], q_words=1):
        task_type = 'select_sent_word'
        task_text = text
        task_object = [text]
        task_options = [text]
        task_answer = [text]
        task_result = ['']
        task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'

        q_words_fact = 0
        q_all_words = 0
        new_sent_1, new_sent_2 = text, text
        i=5
        for token in self.__nlp(text):
            q_all_words += 1
            if (token.pos_ in pos) and (q_words_fact < q_words):
                m, n = np.random.randint(0, i, 2)

                new_word_1 = self.__model.most_similar(token.text.lower(), topn=i)[m][0]
                new_word_2 = self.__model.most_similar(positive = [token.text.lower(), 'bad'],
                                                negative = ['good'],
                                                topn=i)[n][0]

                new_word_1 = new_word_1.title() if token.text.istitle() else new_word_1
                new_word_2 = new_word_2.title() if token.text.istitle() else new_word_2

                new_sent_1 = new_sent_1.replace(token.text, new_word_1)
                new_sent_2 = new_sent_2.replace(token.text, new_word_2)
                q_words_fact += 1

        # –§–∏–ª—å—Ç—Ä, —á—Ç–æ–±—ã –Ω–µ –ø–æ—è–≤–ª—è–ª–∏—Å—å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –º–µ–Ω–µ–µ 3—Ö —Ç–æ–∫–µ–Ω–æ–≤, 
        # –∏ –≤ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç —Å–ª–æ–≤–∞ –Ω—É–∂–Ω–æ–π —á–∞—Å—Ç–∏ —Ä–µ—á–∏
        if q_words_fact > 0 and q_all_words > 3 and len(text) < 100:
            task_options.append(new_sent_1)
            task_options.append(new_sent_2)
            random.shuffle(task_options)
        else:
            task_object = np.nan
            task_options = np.nan
            task_answer = np.nan
            task_result = np.nan
            task_description = np.nan

        return {'raw' : text,
                'task_type' : task_type,
                'task_text' : task_text,
                'task_object' : task_object,
                'task_options' : task_options,
                'task_answer' : task_answer,
                'task_result' : task_result,
                'task_description' : task_description,
                'task_total': 0
                }
    
    # –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –Ω—É–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
# –ï—Å—Ç—å –≤—ã–±–æ—Ä, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å, –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–∫–∞–∑–∞—Ç—å –¥–æ–ª—é –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫—É
    def fill_words_in_the_gaps(self, text, pos=['NOUN', 'VERB', 'ADV', 'ADJ'], q_words=1, hint=True):
        task_type = 'fill_words_in_the_gaps'
        task_text = text     
        task_object = []     
        task_options = []    
        task_answer = []     
        task_result = []     
        task_description = '–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ (–≤–º–µ—Å—Ç–µ —Å –ø–µ—Ä–≤–æ–π –±—É–∫–≤–æ–π, –µ—Å–ª–∏ –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ø–æ–¥—Å–∫–∞–∑–∫–∞)'
        # –ø—Ä–æ—Ö–æ–¥ –ø–æ —Ç–æ–∫–µ–Ω–∞–º —Å –ø–æ–º–æ—â—å—é pyinflect
        count_words = 0
        count_missing_words = 0
        for token in self.__nlp(text):
            count_words += 1
            if token.pos_ in pos and count_missing_words < q_words:
                if hint:
                    task_text = task_text.replace(token.text, token.text[0]+'____')
                else:
                    task_text = task_text.replace(token.text, '_____')
                count_missing_words += 1
                task_object.append(token.text)
                task_answer.append(token.text)
                task_result.append('')

        if count_missing_words < 1:
            return {'raw' : text,
                    'task_type' : task_type,
                    'task_text' : text,
                    'task_object' : np.nan,
                    'task_options' : np.nan,
                    'task_answer' : np.nan,
                    'task_result' : np.nan,
                    'task_description' : np.nan,
                    'task_total': np.nan
                   }
        else:
            return {'raw' : text,
                    'task_type' : task_type,
                    'task_text' : task_text,
                    'task_object' : task_object,
                    'task_options' : task_options,
                    'task_answer' : task_answer,
                    'task_result' : task_result,
                    'task_description' : task_description,
                    'task_total': 0
                   }
    
    def sent_with_no_exercises(self, text):
        return {'raw' : text,
                'task_type' : 'sent_with_no_exercises',
                'task_text' : text,
                'task_object' : np.nan,
                'task_options' : np.nan,
                'task_answer' : np.nan,
                'task_result' : np.nan,
                'task_description' : '–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –±–µ–∑ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è',
                'task_total': np.nan
                }
    
    # –§—É–Ω–∫—Ü–∏—è, —Å–æ–∑–¥–∞—é—â–∞—è –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –≤—Å–µ—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ 
    def create_lesson(self, df, start_row=1, q_task=20, 
                      list_of_exercises=[True, True, True, True, True, True, True], q_words=[1, 1, 1, 1, 1, 1, 1]):
        # –° —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —ç—Ç–æ 1, –∞ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ - 0, –ø–æ—ç—Ç–æ–º—É –≤—ã—á—Ç–µ–º 1
        start_row = start_row-1
        q_task_fact = 0
        lesson_tasks = pd.DataFrame(columns=['row_num', 'raw', 'task_type', 'task_text', 'task_object', 'task_options', 
                                      'task_answer', 'task_result', 'task_description', 'task_total'])

        #–î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
        for i in range(start_row, len(df)):
            mark = 0
            if q_task_fact < q_task:
                row_tasks = pd.DataFrame(columns=['raw', 'task_type', 'task_text', 'task_object', 'task_options', 
                                                  'task_answer', 'task_result', 'task_description', 'task_total'])
                if list_of_exercises[0]:
                    row_tasks.loc[mark] = self.select_word_syn_ant(df.loc[i, 'raw'], q_words=q_words[0])
                    mark += 1
                if list_of_exercises[1]:
                    row_tasks.loc[mark] = self.select_word_adv(df.loc[i, 'raw'])
                    mark += 1
                if list_of_exercises[2]:
                    row_tasks.loc[mark] = self.select_word_verb(df.loc[i, 'raw'])
                    mark += 1
                if list_of_exercises[3]:
                    row_tasks.loc[mark] = self.select_memb_groups(df.loc[i, 'raw'], q_words=q_words[3])
                    mark += 1
                if list_of_exercises[4]:
                    row_tasks.loc[mark] = self.select_sent_verb(df.loc[i, 'raw'])
                    mark += 1
                if list_of_exercises[5]:
                    row_tasks.loc[mark] = self.select_sent_word(df.loc[i, 'raw'], q_words=q_words[5])
                    mark += 1
                if list_of_exercises[6]:
                    row_tasks.loc[mark] = self.fill_words_in_the_gaps(df.loc[i, 'raw'], q_words=q_words[6])
                    mark += 1
                row_tasks.loc[mark] = self.sent_with_no_exercises(df.loc[i, 'raw'])
                row_tasks = row_tasks[row_tasks['raw'].isna() == False]
                row_tasks['row_num'] = df.loc[i, 'row_num']
                row_tasks = row_tasks[row_tasks['task_description'].isna() == False]

                # –ï—Å–ª–∏ —Ö–æ—Ç—å –∫–∞–∫–æ–µ-—Ç–æ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –≤—ã–≥—Ä—É–∑–∏–ª–æ—Å—å, —Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø–ª—é—Å 1 –≤ —Å—á–µ—Ç—á–∏–∫
                if len(row_tasks[row_tasks['task_type'] != 'sent_with_no_exercises']) != 0:
                    q_task_fact += 1
                lesson_tasks = pd.concat([lesson_tasks, row_tasks], ignore_index=True)

        return lesson_tasks
    
    def create_default_lesson(self, df):
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏, –ø–æ—Ç–æ–º —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–æ–∫–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤–æ–µ –ø–æ–ø–∞–≤—à–µ–µ—Å—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ
        new_df = df[df['task_type'] != 'sent_with_no_exercises'].sample(frac=1).sort_values(by='row_num')
        new_df = new_df.groupby('row_num').agg('first').reset_index()

        df_with_no_exer = df[~df['row_num'].isin(new_df['row_num'].unique())]
        new_df = pd.concat([new_df, df_with_no_exer], ignore_index=True).sort_values('row_num').reset_index(drop=True)

        return new_df
    
    def show_result_table(self, df):
        new_df = df[['task_description', 'task_text', 'task_answer', 'task_result', 'task_total']]
        new_df['task_answer'] = new_df['task_answer'].astype('str')
        new_df['task_result'] = new_df['task_result'].astype('str')
        new_df.columns = ['–¢–∏–ø –∑–∞–¥–∞–Ω–∏—è', '–¢–µ–∫—Å—Ç —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è', '–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç', '–û—Ç–≤–µ—Ç –Ω–∞ –∑–∞–¥–∞–Ω–∏–µ', '–†–µ–∑—É–ª—å—Ç–∞—Ç']
        return new_df
    
    def show_result_by_task_type(self, df):
        new_df = (df[df['task_type'] != 'sent_with_no_exercises']
                  .groupby('task_description')['task_total']
                  .agg(['count', 'sum']))
        new_df['wrong_answers'] = new_df['count'] - new_df['sum']
        new_df['result'] = new_df['sum'].astype('int').astype('str') + ' / ' + new_df['count'].astype('int').astype('str')
        new_df = new_df.sort_values(by='wrong_answers', ascending=False)
        new_df = new_df.drop(['count', 'sum', 'wrong_answers'], axis=1).reset_index()
        new_df.columns = ['–¢–∏–ø —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è', '–†–µ–∑—É–ª—å—Ç–∞—Ç']
        return new_df
    
    def result_interpretation(self, df):
        correct_cnt = int(df['task_total'].sum())
        all_cnt = int(df.loc[df['task_type'] != 'sent_with_no_exercises', 'task_total'].count())
        correct_prop = round(correct_cnt / all_cnt * 100, 0)
        
        result_info = ('–í–∞—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç: ' + str(correct_cnt) + ' / ' + str(all_cnt) + ' (' + str(correct_prop) + '%)')
        result_comment = ''
        if correct_prop == 100:
            result_comment = '–ê –∫–∞–∫ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –∫ –≤–∞–º –Ω–∞ —É—Ä–æ–∫ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ? ü§ì'
        elif correct_prop >= 90:
            result_comment = '–û—Ç–ª–∏—á–Ω–æ! –ù—É–∂–Ω–æ —Å–æ–≤—Å–µ–º –Ω–µ–º–Ω–æ–≥–æ, —á—Ç–æ–±—ã –¥–æ–±–∏—Ç—å—Å—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ üßó‚Äç‚ôÇÔ∏è'
        elif correct_prop >= 70:
            result_comment = '–û—á–µ–Ω—å –Ω–µ–ø–ª–æ—Ö–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ù–æ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ–º–∞—Ö —Å—Ç–æ–∏—Ç –µ—â–µ –ø–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—èüë®‚Äçüéì'
        else:
            result_comment = '–°—Ç–æ–∏—Ç –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º ü§î'
        
        result_mistakes = ''
        if correct_cnt != all_cnt:
            task_with_mistakes = str(df.loc[df['task_total'] == 0, 'task_description'].unique())
            result_mistakes = '–û—à–∏–±–∫–∏ –±—ã–ª–∏ –≤ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è—Ö —Å–ª–µ–¥—É—é—â–∏—Ö —Ç–∏–ø–æ–≤: ' + task_with_mistakes + '. –í —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –ø–æ–ø—Ä–æ–±—É–π —Å–¥–µ–ª–∞—Ç—å —É–ø–æ—Ä –∏–º–µ–Ω–Ω–æ –Ω–∞ —Ç–∞–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è'
        
        return result_info + "\n" + result_comment + "\n" + result_mistakes
    
    def display_dataset(self, df):
        new_df = df
        for column in df.columns:
            new_df[column] = new_df[column].astype('str')
        return new_df
    