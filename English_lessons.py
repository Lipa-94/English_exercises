# # –ü—Ä–æ–µ–∫—Ç: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É —è–∑—ã–∫—É

# ## –ó–∞–≥—Ä—É–∑–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
import re
import random

import pandas as pd
import numpy as np
import gensim
import spacy
import pyinflect
import streamlit  as st

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import en_core_web_sm
import gensim.downloader as api

# –º–∞–ª–∞—è –º–æ–¥–µ–ª—å spacy
nlp = en_core_web_sm.load()

# –º–∞–ª–∞—è –º–æ–¥–µ–ª—å glove wiki
# –≤–Ω–∏–º–∞–Ω–∏–µ - –æ—á–µ–Ω—å –¥–æ–ª–≥–æ —Å–∫–∞—á–∏–≤–∞–µ—Ç, –µ—Å–ª–∏ –æ–Ω–∞ –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
model = api.load("glove-wiki-gigaword-100")


# ## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
def open_text(text):
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
    paragraphs = text.split("\n")
    dataset = pd.DataFrame({'raw': paragraphs})
    dataset = dataset[dataset['raw'] != '']
    
    # –ü–µ—Ä–µ–Ω–æ—Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
    nlp = spacy.load("en_core_web_sm")
    rows_list = []
    list_text = dataset['raw'].values
    for i in list_text:
        doc = nlp(i)
        a = [sent.text.strip() for sent in doc.sents]
        for j in a:
            rows_list.append(j)
    df = pd.DataFrame(rows_list, columns=['raw'])
    
    return df

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
def open_file(file):
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
    dataset = pd.read_csv(file, names=['raw'], delimiter="\t")
    
    # –ü–µ—Ä–µ–Ω–æ—Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
    nlp = spacy.load("en_core_web_sm")
    rows_list = []
    list_text = dataset['raw'].values
    for i in list_text:
        doc = nlp(i)
        a = [sent.text.strip() for sent in doc.sents]
        for j in a:
            rows_list.append(j)
    df = pd.DataFrame(rows_list, columns=['raw'])
    
    return df

# –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–±–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é –∏–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é –∫–∞–≤—ã—á–∫—É –≤ —Ç–µ–∫—Å—Ç
def quots_func(text):
    # –°—á–∏—Ç–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –≤ —Å—Ç—Ä–æ–∫–µ. –°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ, —Å–∫–æ–ª—å–∫–æ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö –∏ —Å–∫–æ–ª—å–∫–æ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö
    count_q = 0
    count_q_open = 0
    
    # –ë–µ—Ä–µ–º –∏–∑ –≤—Å–µ–π —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏ –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –∏ —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    for i in range(len(text)):
        if text[i] == '"':
            count_q += 1
            try:
                if re.sub(r'[^a-zA-Z]', '', text[i+1]) != '':
                    count_q_open += 1 
            except:
                pass
    
    # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö –∫–∞–≤—ã—á–µ–∫
    count_q_close = count_q - count_q_open
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É: –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö —Å—Ç—Ä–æ–∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
    if count_q_close == count_q_open:
        return text
    # –ï—Å–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö —Å–∫–æ–±–æ–∫ –±–æ–ª—å—à–µ, —á–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–≤—ã—á–∫—É –≤ –Ω–∞—á–∞–ª–æ
    elif count_q_close > count_q_open:
        return '"'+text
    # –ï—Å–ª–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö —Å–∫–æ–±–æ–∫ –±–æ–ª—å—à–µ, —á–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–≤—ã—á–∫—É –≤ –∫–æ–Ω–µ—Ü
    else:
        return text+'"'

# –§—É–Ω–∫—Ü–∏—è, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—é—â–∞—è –¥–∞—Ç–∞—Å–µ—Ç: –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç—Å—è –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫, –∫–∞–≤—ã—á–∫–∏
def beautify_text(df):
    # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ–º—Å—è —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã, –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –µ–µ –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å—Ç—Ä–æ–∫–µ, 
    # —Ç.–∫. —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —ç—Ç–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä—è–º–æ–π —Ä–µ—á–∏
    df['shift_raw'] = df['raw'].shift(periods=-1, fill_value='')
    df['raw_regex'] = df['raw'].str[:1].str.replace(r'[^a-z]', '', regex=True)
    df['shift_raw_regex'] = df['raw_regex'].shift(periods=-1, fill_value='')
    df.loc[df['shift_raw_regex'] != '', 'raw'] = (df.loc[df['shift_raw_regex'] != '', 'raw'] + 
                                                  '" ' + 
                                                  df.loc[df['shift_raw_regex'] != '', 'shift_raw'])
    df = df[df['raw_regex'] == '']
    df = df.drop(['shift_raw', 'raw_regex', 'shift_raw_regex'], axis=1)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–≤—ã—á–µ–∫
    df['raw'] = df['raw'].apply(quots_func)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º
    df = df.reset_index()
    df = df.rename(columns={"index": "row_num"})
    
    return df


# ## –§—É–Ω–∫—Ü–∏–∏, —Å–æ–∑–¥–∞—é—â–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è

# ### –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —Å –≤—ã–±–æ—Ä–æ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞

# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ø—Ä–æ–ø—É—Å–∫ —Å–ª–æ–≤–∞, –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, —Å–∏–Ω–æ–Ω–∏–º, –∞–Ω—Ç–æ–Ω–∏–º
# –ü—Ä–æ–±–ª–µ–º–∞: –∏–Ω–æ–≥–¥–∞ –≤—ã–≤–æ–¥–∏—Ç —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
def select_word_syn_ant(text, pos=['NOUN', 'VERB', 'ADJ', 'ADV'], q_words=1):
    task_type = 'select_word_syn_ant'
    task_text = text
    task_object = []
    task_options = []
    task_answer = []
    task_result = []
    task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ:'
    
    # –î–ª—è –∑–∞–¥–∞–Ω–∏—è –≤—ã–±–∏—Ä–∞—é—Ç—Å—è —Ä–∞–Ω–¥–æ–º–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ q_words
    for token in nlp(text):
        if token.pos_ in pos:
            task_object.append(token.text)
    
    if len(task_object) > 1:
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏ –ø—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º –∏—Ö –≤ —Ç–æ–º –ø–æ—Ä—è–¥–∫–µ, –≤ –∫–∞–∫–æ–º –æ–Ω–∏ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –∫ —Ç–µ–∫—Å—Ç–µ
        order = {number:task for number,task in enumerate(task_object)}
        task_object = random.sample(task_object, k=min(q_words, len(task_object)))
        order_list = []
        for i in task_object:
            order_list.append(list(order.keys())[list(order.values()).index(i)])
        order_list, task_object = zip(*sorted(zip(order_list, task_object)))
        task_object = list(task_object)
        
        for token in task_object:
            task_text = task_text.replace(token, '_____')
            task_answer.append(token)
            task_result.append('')

        task_options = [[token] for token in task_object]

        for token in task_options:
            token_new = token[0].lower()

            # –ò—â–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
            synonyms = model.similar_by_word(token_new)
            synonyms = [ _[0] for _ in synonyms]
            synonyms = [_.text for _ in nlp(' '.join(synonyms)) if not _.is_stop]
            try:
                if synonyms[0] == token_new:
                    token.append(synonyms[1].title() if token[0].istitle() else synonyms[1])
                else:
                    token.append(synonyms[0].title() if token[0].istitle() else synonyms[0])
            except:
                pass

            # –ò—â–µ–º –∞–Ω—Ç–æ–Ω–∏–º—ã
            antonyms = model.most_similar(positive=[token_new, 'bad'], negative=['good'])
            antonyms = [ _[0] for _ in antonyms]
            antonyms = [_.text for _ in nlp(' '.join(antonyms)) if not _.is_stop]
            try:
                if antonyms[0] == token_new or antonyms[0].title() == token[1] or antonyms[0] == token[1]:
                    token.append(antonyms[1].title() if token[0].istitle() else antonyms[1])
                else:
                    token.append(antonyms[0].title() if token[0].istitle() else antonyms[0])
            except:
                pass

            random.shuffle(token)
    else:
        task_object = np.nan
        task_options = np.nan
        task_answer = np.nan
        task_result = np.nan
        task_description = np.nan
                               
    return {'raw' : text,
            'task_type' : task_type,
            'task_text' : task_text,
            'task_object' : task_object,
            'task_options' : task_options,
            'task_answer' : task_answer,
            'task_result' : task_result,
            'task_description' : task_description
            }

# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ø—Ä–æ–ø—É—Å–∫ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ, –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞: 3 —Ñ–æ—Ä–º—ã –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ
def select_word_adv(text):
    task_text = text
    task_type = 'select_word_adv'
    task_object = []
    task_options = []
    task_answer = []
    task_result = []
    task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ:'
    
    # –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å –ø–æ–º–æ—â—å—é pyinflect
    for token in nlp(text):
        # –≤—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ, –Ω–æ —Ç–æ–ª—å–∫–æ —Ç–µ, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –≤—Å–µ 3 —Ñ–æ—Ä–º—ã
        if (token.pos_=='ADJ' and 
            token._.inflect('JJ') != None and 
            token._.inflect('JJR') != None and 
            token._.inflect('JJS') != None):
            task_text = task_text.replace(token.text, '_____')
            task_object.append(token)
            task_answer.append(token)
            task_result.append('')
            task_adv_options = []
            task_adv_options.append(token._.inflect('JJ'))  # JJ      Adjective
            task_adv_options.append(token._.inflect('JJR')) # JJR     Adjective, comparative
            task_adv_options.append(token._.inflect('JJS')) # JJS     Adjective, superlative
            task_options.append(task_adv_options)
    
    if task_object == []:
        task_object = np.nan
        task_options = np.nan
        task_answer = np.nan
        task_result = np.nan
        task_description = np.nan
        
    return {'raw' : text,
            'task_type' : task_type,
            'task_text' : task_text,
            'task_object' : task_object,
            'task_options' : task_options,
            'task_answer' : task_answer,
            'task_result' : task_result,
            'task_description' : task_description
            }

# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ø—Ä–æ–ø—É—Å–∫ –≥–ª–∞–≥–æ–ª–∞, –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞: 3 —Ñ–æ—Ä–º—ã –≥–ª–∞–≥–æ–ª–∞
def select_word_verb(text):
    task_type = 'select_verb_form'
    task_text = text
    task_object = []
    task_options = []
    task_answer = []
    task_result = []
    task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –≥–ª–∞–≥–æ–ª–∞:'
    
    # –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –≥–ª–∞–≥–æ–ª–∞ —Å –ø–æ–º–æ—â—å—é pyinflect
    for token in nlp(text):
        if (token.pos_=='VERB'):
            task_text = task_text.replace(token.text, '_____')
            task_object.append(token)
            task_answer.append(token)
            task_result.append('')
            task_adv_options = []
            for i in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']:
                # * VB      Verb, base form
                # * VBD     Verb, past tense
                # * VBG     Verb, gerund or present participle
                # * VBN     Verb, past participle
                # * VBP     Verb, non-3rd person singular present
                # * VBZ     Verb, 3rd person singular present
                # * MD      Modal'''
                if token._.inflect(i) not in task_adv_options and token._.inflect(i) != None:
                    task_adv_options.append(token._.inflect(i)) 
            task_options.append(task_adv_options)
    
    if task_object == []:
        task_object = np.nan
        task_options = np.nan
        task_answer = np.nan
        task_result = np.nan
        task_description = np.nan
        
    return {'raw' : text,
            'task_type' : task_type,
            'task_text' : task_text,
            'task_object' : task_object,
            'task_options' : task_options,
            'task_answer' : task_answer,
            'task_result' : task_result,
            'task_description' : task_description
            }


# ### –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã, –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞: –Ω–∞–∑–≤–∞–Ω–∏—è —á–∞—Å—Ç–µ–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
def select_memb_groups(text, q_words=1):
    task_type = 'select_memb_main_sec'
    task_text = text
    task_object = []
    task_options = []
    task_answer = []
    task_result = []
    task_description = '–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —á–µ–º —è–≤–ª—è–µ—Ç—Å—è –≤—ã–¥–µ–ª–µ–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞?'
    
    # –î–ª—è –∑–∞–¥–∞–Ω–∏—è –≤—ã–±–∏—Ä–∞—é—Ç—Å—è —Ä–∞–Ω–¥–æ–º–Ω—ã–µ chunk –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ q_words
    for chunk in nlp(text).noun_chunks:
        task_object.append(chunk)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∏ –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    if len(task_object) > 1:
        order = {number:task for number,task in enumerate(task_object)}
        task_object = random.sample(task_object, k=min(q_words, len(task_object)))
        order_list = []
        for i in task_object:
            order_list.append(list(order.keys())[list(order.values()).index(i)])
        order_list, task_object = zip(*sorted(zip(order_list, task_object)))
        task_object = list(task_object)

        for chunk in task_object:
            task_answer.append(spacy.explain(chunk.root.dep_))
            task_options.append('')
            task_result.append('')
        task_object = [chunk.text for chunk in task_object]

        # –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏–∑ –≤—Å–µ—Ö –æ–ø–∏—Å–∞–Ω–∏–π —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –∏–∑ task_answer
        # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ 3-—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞, —Ç–æ –¥–æ–∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        unique_answers = list(set(task_answer))
        dep_list = ['clausal modifier of noun (adjectival clause)', 'adjectival complement', 'adverbial clause modifier', 
                'adverbial modifier', 'agent', 'adjectival modifier', 'appositional modifier', 'attribute', 'auxiliary', 
                'auxiliary (passive)', 'case marking', 'coordinating conjunction', 'clausal complement', 'compound', 
                'conjunct', 'copula', 'clausal subject', 'clausal subject (passive)', 'dative', 'unclassified dependent', 
                'determiner', 'direct object', 'expletive', 'interjection', 'marker', 'meta modifier', 'negation modifier', 
                'noun compound modifier', 'noun phrase as adverbial modifier', 'nominal subject', 'nominal subject (passive)', 
                'object predicate', 'object', 'oblique nominal', 'complement of preposition', 'object of preposition', 
                'possession modifier', 'pre-correlative conjunction', 'prepositional modifier', 'particle', 'punctuation', 
                'modifier of quantifier', 'relative clause modifier', 'root', 'open clausal complement']
        for i in unique_answers:
            try:
                dep_list.remove(i)
            except:
                pass
        if len(unique_answers) == 2:
            unique_answers.append(random.choice(dep_list))
        elif len(unique_answers) == 1:
            unique_answers.extend(random.sample(dep_list, k=2))
        random.shuffle(unique_answers)
        task_options = [unique_answers for _ in task_options]
    else:
        task_object = np.nan
        task_options = np.nan
        task_answer = np.nan
        task_result = np.nan
        task_description = np.nan
    
    return {'raw' : text,
            'task_type' : task_type,
            'task_text' : task_text,
            'task_object' : task_object,
            'task_options' : task_options,
            'task_answer' : task_answer,
            'task_result' : task_result,
            'task_description' : task_description
            }


# ### –§—É–Ω–∫—Ü–∏–∏ c–æ–∑–¥–∞–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –≤—ã–±–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ

# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä, –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞–º–∏ –≥–ª–∞–≥–æ–ª–∞
def select_sent_verb(text):
    task_type = 'select_sent_verb'
    task_text = text
    task_object = [text]
    task_options = [text]
    task_answer = [text]
    task_result = ['']
    task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º–æ–π –≥–ª–∞–≥–æ–ª–∞:'
    
    new_sent_1, new_sent_2 = text, text
    i=5
    count_verbs = 0
    for token in nlp(text):
        if (token.pos_=='VERB'):
            # –°–æ—Å—Ç–∞–≤–ª—è–µ–º –ª–∏—Å—Ç —Ñ–æ—Ä–º —Å–ª–æ–≤, –Ω–µ –≤–∫–ª—é—á–∞—é—â–∏–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –≥–ª–∞–≥–æ–ª–∞
            verb_forms = []
            for i in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']:
                # * VB      Verb, base form
                # * VBD     Verb, past tense
                # * VBG     Verb, gerund or present participle
                # * VBN     Verb, past participle
                # * VBP     Verb, non-3rd person singular present
                # * VBZ     Verb, 3rd person singular present
                # * MD      Modal
                if token._.inflect(i) != token.text and token._.inflect(i) != None and token._.inflect(i) not in verb_forms:
                    verb_forms.append(token._.inflect(i))
    
            new_word_1 = random.choice(verb_forms)
            new_word_1 = new_word_1.title() if token.text.istitle() else new_word_1
            new_sent_1 = new_sent_1.replace(token.text, new_word_1)
            verb_forms.remove(new_word_1)
            new_word_2 = random.choice(verb_forms)
            new_word_2 = new_word_1.title() if token.text.istitle() else new_word_2
            new_sent_2 = new_sent_2.replace(token.text, new_word_2)
            count_verbs += 1
            
    if count_verbs > 0:
        task_options.append(new_sent_1)
        task_options.append(new_sent_2)
        random.shuffle(task_options)
    else:
        task_object = np.nan
        task_options = np.nan
        task_answer = np.nan
        task_result = np.nan
        task_description = np.nan
    
    return {'raw' : text,
            'task_type' : task_type,
            'task_text' : task_text,
            'task_object' : task_object,
            'task_options' : task_options,
            'task_answer' : task_answer,
            'task_result' : task_result,
            'task_description' : task_description
            }

# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä, –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å–ª–æ–≤–æ–º, —Å —Å–∏–Ω. –∏–ª–∏ –∞–Ω—Ç.
def select_sent_word(text, pos=['NOUN', 'VERB', 'ADV', 'ADJ'], q_words=1):
    task_type = 'select_sent_word'
    task_text = text
    task_object = [text]
    task_options = [text]
    task_answer = [text]
    task_result = ['']
    task_description = '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:'
    
    q_words_fact = 0
    q_all_words = 0
    new_sent_1, new_sent_2 = text, text
    i=5
    for token in nlp(text):
        q_all_words += 1
        if (token.pos_ in pos) and (q_words_fact < q_words):
            m, n = np.random.randint(0, i, 2)
            
            new_word_1 = model.most_similar(token.text.lower(), topn=i)[m][0]
            new_word_2 = model.most_similar(positive = [token.text.lower(), 'bad'],
                                            negative = ['good'],
                                            topn=i)[n][0]

            new_word_1 = new_word_1.title() if token.text.istitle() else new_word_1
            new_word_2 = new_word_2.title() if token.text.istitle() else new_word_2
        
            new_sent_1 = new_sent_1.replace(token.text, new_word_1)
            new_sent_2 = new_sent_2.replace(token.text, new_word_2)
            q_words_fact += 1
    
    # –§–∏–ª—å—Ç—Ä, —á—Ç–æ–±—ã –Ω–µ –ø–æ—è–≤–ª—è–ª–∏—Å—å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –º–µ–Ω–µ–µ 3—Ö —Ç–æ–∫–µ–Ω–æ–≤, 
    # –∏ –≤ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç —Å–ª–æ–≤–∞ –Ω—É–∂–Ω–æ–π —á–∞—Å—Ç–∏ —Ä–µ—á–∏
    if q_words_fact > 0 and q_all_words > 3:
        task_options.append(new_sent_1)
        task_options.append(new_sent_2)
        random.shuffle(task_options)
    else:
        task_object = np.nan
        task_options = np.nan
        task_answer = np.nan
        task_result = np.nan
        task_description = np.nan
    
    return {'raw' : text,
            'task_type' : task_type,
            'task_text' : task_text,
            'task_object' : task_object,
            'task_options' : task_options,
            'task_answer' : task_answer,
            'task_result' : task_result,
            'task_description' : task_description
            }


# ### –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞, —á–∞—Å—Ç–∏ —Ä–µ—á–∏

# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—é—â–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –Ω—É–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
# –ï—Å—Ç—å –≤—ã–±–æ—Ä, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å, –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–∫–∞–∑–∞—Ç—å –¥–æ–ª—é –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫—É
def fill_words_in_the_gaps(text, pos=['NOUN', 'VERB', 'ADV', 'ADJ'], q_words=1, hint=True):
    task_type = 'fill_words_in_the_gaps'
    task_text = text     
    task_object = []     
    task_options = []    
    task_answer = []     
    task_result = []     
    task_description = '–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ'
    # –ø—Ä–æ—Ö–æ–¥ –ø–æ —Ç–æ–∫–µ–Ω–∞–º —Å –ø–æ–º–æ—â—å—é pyinflect
    count_words = 0
    count_missing_words = 0
    for token in nlp(text):
        count_words += 1
        if token.pos_ in pos and count_missing_words < q_words:
            if hint:
                task_text = task_text.replace(token.text, token.text[0]+'____')
            else:
                task_text = task_text.replace(token.text, '_____')
            count_missing_words += 1
            task_object.append(token.text)
            task_answer.append(token.text)
            task_result.append('')
    
    if count_missing_words < 1:
        return {'raw' : text,
                'task_type' : task_type,
                'task_text' : text,
                'task_object' : np.nan,
                'task_options' : np.nan,
                'task_answer' : np.nan,
                'task_result' : np.nan,
                'task_description' : np.nan
               }
    else:
        return {'raw' : text,
                'task_type' : task_type,
                'task_text' : task_text,
                'task_object' : task_object,
                'task_options' : task_options,
                'task_answer' : task_answer,
                'task_result' : task_result,
                'task_description' : task_description
               }


# ### –ü—Ä–æ–ø—É—Å–∫ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è

# –ü—Ä–æ–ø—É—Å–∫ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
def sent_with_no_exercises(text):
    return {'raw' : text,
            'task_type' : 'sent_with_no_exercises',
            'task_text' : text,
            'task_object' : np.nan,
            'task_options' : np.nan,
            'task_answer' : np.nan,
            'task_result' : np.nan,
            'task_description' : '–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –±–µ–∑ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è'
            }


# ## –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è–º–∏

# –§—É–Ω–∫—Ü–∏—è, —Å–æ–∑–¥–∞—é—â–∞—è –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –≤—Å–µ—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ 
def create_lesson(df, start_row=1, q_task=20):
    # –° —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —ç—Ç–æ 1, –∞ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ - 0, –ø–æ—ç—Ç–æ–º—É –≤—ã—á—Ç–µ–º 1
    start_row = start_row-1
    q_task_fact = 0
    lesson_tasks = pd.DataFrame(columns=['row_num', 'raw', 'task_type', 'task_text', 'task_object', 'task_options', 
                                  'task_answer', 'task_result', 'task_description'])
    
    #–î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
    for i in range(start_row, len(df)):
        mark = 0
        if q_task_fact < q_task:
            row_tasks = pd.DataFrame(columns=['raw', 'task_type', 'task_text', 'task_object', 'task_options', 
                                              'task_answer', 'task_result', 'task_description'])
            row_tasks.loc[0] = select_word_syn_ant(df.loc[i, 'raw'])
            row_tasks.loc[1] = select_word_adv(df.loc[i, 'raw'])
            row_tasks.loc[2] = select_word_verb(df.loc[i, 'raw'])
            row_tasks.loc[3] = select_memb_groups(df.loc[i, 'raw'])
            row_tasks.loc[4] = select_sent_verb(df.loc[i, 'raw'])
            row_tasks.loc[5] = select_sent_word(df.loc[i, 'raw'])
            row_tasks.loc[6] = fill_words_in_the_gaps(df.loc[i, 'raw'])
            row_tasks.loc[7] = sent_with_no_exercises(df.loc[i, 'raw'])
            row_tasks = row_tasks[row_tasks['raw'].isna() == False]
            row_tasks['row_num'] = df.loc[i, 'row_num']
            row_tasks = row_tasks[row_tasks['task_description'].isna() == False]
            
            # –ï—Å–ª–∏ —Ö–æ—Ç—å –∫–∞–∫–æ–µ-—Ç–æ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –≤—ã–≥—Ä—É–∑–∏–ª–æ—Å—å, —Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø–ª—é—Å 1 –≤ —Å—á–µ—Ç—á–∏–∫
            if len(row_tasks[row_tasks['task_type'] != 'sent_with_no_exercises']) != 0:
                q_task_fact += 1
            lesson_tasks = pd.concat([lesson_tasks, row_tasks], ignore_index=True)
        
    return lesson_tasks

# –§—É–Ω–∫—Ü–∏—è, —Å–æ–∑–¥–∞—é—â–∞—è –Ω–∞–±–æ—Ä —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –¥–ª—è —É—Ä–æ–∫–∞. –í—ã–≤–æ–¥—è—Ç—Å—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω–æ
def create_default_lesson(df):
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏, –ø–æ—Ç–æ–º —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–æ–∫–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤–æ–µ –ø–æ–ø–∞–≤—à–µ–µ—Å—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ
    new_df = df[df['task_type'] != 'sent_with_no_exercises'].sample(frac=1).sort_values(by='row_num')
    new_df = new_df.groupby('row_num').agg('first').reset_index()
    
    df_with_no_exer = df[~df['row_num'].isin(new_df['row_num'].unique())]
    new_df = pd.concat([new_df, df_with_no_exer], ignore_index=True).sort_values('row_num').reset_index(drop=True)
    
    return new_df

# –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∫–∞–∫–æ–≥–æ —Ç–∏–ø–∞ –º–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–∞–≤–∏—Ç—å
def get_options(lesson_dataset, row_num):
    return lesson_dataset.loc[lesson_dataset['row_num'] == row_num, 'task_description'].unique()

# –§—É–Ω–∫—Ü–∏—è, —Å –ø–æ–º–æ—â—å—é –∫–æ—Ç–æ—Ä–æ–π –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –æ–¥–∏–Ω —Ç–∏–ø —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –Ω–∞ –¥—Ä—É–≥–æ–π
def change_task_type(lesson, row_num, new_type, pos=['NOUN', 'VERB', 'ADV', 'ADJ'], q_words=1):
    new_task = pd.DataFrame(columns=['raw', 'task_type', 'task_text', 'task_object', 'task_options', 
                                      'task_answer', 'task_result', 'task_description'])
    display()
    if new_type == 'select_word_syn_ant':
        new_task.loc[0] = select_word_syn_ant(lesson.loc[lesson['row_num'] == row_num, 'raw'].values[0], pos=pos, q_words=q_words)
    elif new_type == 'select_word_adv':
        new_task.loc[0] = select_word_adv(lesson.loc[lesson['row_num'] == row_num, 'raw'].values[0])
    elif new_type == 'select_word_verb':
        new_task.loc[0] = select_word_verb(lesson.loc[lesson['row_num'] == row_num, 'raw'].values[0])
    elif new_type == 'select_memb_groups':
        new_task.loc[0] = select_memb_groups(lesson.loc[lesson['row_num'] == row_num, 'raw'].values[0], q_words=q_words)
    elif new_type == 'select_sent_verb':
        new_task.loc[0] = select_sent_verb(lesson.loc[lesson['row_num'] == row_num, 'raw'].values[0])
    elif new_type == 'select_sent_word':
        new_task.loc[0] = select_sent_word(lesson.loc[lesson['row_num'] == row_num, 'raw'].values[0], pos=pos, q_words=q_words)
    elif new_type == 'fill_words_in_the_gaps':
        new_task.loc[0] = fill_words_in_the_gaps(lesson.loc[lesson['row_num'] == row_num, 'raw'].values[0], 
                                                 pos=pos, q_words=q_words, hint=True)
    else:
        new_task.loc[0] = sent_with_no_exercises(lesson.loc[lesson['row_num'] == row_num, 'raw'].values[0])
    
    new_task['row_num'] = row_num
    
    lesson = lesson[lesson['row_num'] != row_num]
    lesson = pd.concat([lesson, new_task]).sort_values('row_num').reset_index(drop=True)
    return lesson

# –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á–µ—Ä–µ–∑ Jupiter Notebook
#df = open_file("Little_Red_Cap_Jacob_and_Wilhelm_Grimm.txt")
#df = beautify_text(df)
# –°–æ–∑–¥–∞–ª–∏ –º–∞—Å—Å–∏–≤ –≤—Å–µ—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
#lesson_dataset = create_lesson(df)
# –í—ã–±—Ä–∞–ª–∏ —Å–ª—É—á–∞–π–Ω—ã–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
#default_lesson = create_default_lesson(lesson_dataset)
#display(default_lesson)
# –í—ã–≤–µ–ª–∏ –≤—Å–µ –æ–ø—Ü–∏–∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –¥–ª—è —Å—Ç—Ä–æ–∫–∏ 2
# print(get_options(lesson_dataset, row_num=2))
# –ó–∞–º–µ–Ω–∏–ª–∏ –≤ —É—Ä–æ–∫–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2 –Ω–∞ fill_words_in_the_gaps
#lesson = change_task_type(default_lesson, 2, 'fill_words_in_the_gaps', pos='ADV', q_words=3)
#display(lesson)


# ## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ –Ω–∞ streamlit

random.seed(123)

st.header('–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É —è–∑—ã–∫—É')

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
st.subheader('–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –∏–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ –≤ –ø–æ–ª–µ —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è')
lesson_file = None
lesson_text = None
tab1, tab2 = st.tabs(["–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª", "–í—Å—Ç–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç"])
with tab1:
    lesson_file = st.file_uploader('nolabel', label_visibility="hidden")
with tab2:
    st.subheader('–í—Å—Ç–∞–≤—å—Ç–µ –≤ –ø–æ–ª–µ —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è')
    lesson_text = st.text_area('nolabel', label_visibility="hidden")

# –ï—Å–ª–∏ —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
save_button = False
dataset = None
if lesson_file is not None and lesson_text != '':
    st.write('–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –æ–ø—Ü–∏—é: –ª–∏–±–æ –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞, –ª–∏–±–æ –≤—Å—Ç–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ–ª–µ')
elif lesson_file is not None:
    dataset = open_file(lesson_file)
    dataset = beautify_text(dataset)
    save_button = st.button('–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è')
elif lesson_text is not None:
    dataset = open_text(lesson_text)
    dataset = beautify_text(dataset)
    save_button = st.button('–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è')
else:
    pass

# –ë—É–¥—É—â–∏–π –±–ª–æ–∫ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏


# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
if save_button:
    st.dataframe(dataset)
    q_task = 20
    start_row = 1
    lesson_dataset = create_lesson(dataset, start_row=start_row, q_task=q_task)
    default_lesson = create_default_lesson(lesson_dataset)
    
    st.subheader('–£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É')
    
    for i in range(len(default_lesson)):
        task = default_lesson.loc[i]
        st.write('–ó–∞–¥–∞–Ω–∏–µ #'+ str(i) + ': ' +str(task['task_description']))
        
        if task['task_type'] == 'sent_with_no_exercises':
            st.write(str(task['task_text']))
        elif task['task_type'] in ['select_sent_verb', 'select_sent_word']:
            task['task_result'] = st.selectbox('nolabel', 
                                               ['‚Äì‚Äì‚Äì'] + task['task_options'], 
                                               label_visibility="hidden",
                                               key = i)
            if task['task_result'] == '‚Äì‚Äì‚Äì' or task['task_result'] == []:
                pass
            elif task['task_result'] == task['task_answer']:
                st.success('', icon="‚úÖ")
            else:
                st.error('', icon="üòü")
        elif task['task_type'] == 'fill_words_in_the_gaps':
            col1, col2 = st.columns(2)
            with col1:
                st.write(str(task['task_text']))
            with col2:
                task['task_result'] = st.text_input(label='–í–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ (–≤–º–µ—Å—Ç–µ —Å –ø–µ—Ä–≤–æ–π –±—É–∫–≤–æ–π, –µ—Å–ª–∏ –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ø–æ–¥—Å–∫–∞–∑–∫–∞)', value='---', key=i)
            if task['task_result'] == '‚Äì‚Äì‚Äì' or task['task_result'] == [] or task['task_result'] == ['‚Äì‚Äì‚Äì']:
                pass
            elif task['task_result'] == task['task_answer']:
                st.success('', icon="‚úÖ")
            else:
                st.error('', icon="üòü")
        else:
            col1, col2 = st.columns(2)
            with col1:
                st.write(str(task['task_text']))
            with col2:
                #st.write(str(task['task_options']))
                for j in range(len(task['task_options'])):
                    option = task['task_options'][j]
                    task['task_result'][j] = st.selectbox('nolabel', 
                                                          ['‚Äì‚Äì‚Äì'] + option, 
                                                          label_visibility="hidden",
                                                          key = ((j+1)*len(default_lesson) + (i+1)))
                    if task['task_result'][j] == '‚Äì‚Äì‚Äì':
                        pass
                    elif task['task_result'][j] == task['task_answer'][j]:
                        st.success('', icon="‚úÖ")
                    else:
                        st.error('', icon="üòü")
        '---'  

# –î–∞–ª—å–Ω–µ–π—à–∏–π –ø–ª–∞–Ω:

# –î–æ–±–∞–≤–∏—Ç—å —Å–∫—Ä—ã–≤–∞–µ–º—ã–π –±–ª–æ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–∫. –í –Ω–µ–º: –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª-–≤–∞ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏, —Å –∫–æ—Ç–æ—Ä–æ–π –∏–¥—É—Ç —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è, –∏ —Ç–∏–ø—ã —É–ø—Ä.
# –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥—Å—á–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π, –∫–∞–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —Ö—É–∂–µ –∏–¥—É—Ç. + –ü–æ–¥—Ä–æ–±–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞. + –§–∞–π–ª —Å —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–æ–π (—á—Ç–æ–±—ã –ø–æ–¥–µ–ª–∏—Ç—å—Å—è)

# –î–æ–±–∞–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —É—á–µ–Ω–∏–∫–∞ (–æ—Ç–∫—Ä—ã—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –Ω–∞ –Ω–µ–π —Ç–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞, –æ—Å–Ω.–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è) –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É —É—á–∏—Ç–µ–ª—è
# –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —É—á–∏—Ç–µ–ª—è –æ—Ç–∫—Ä—ã–≤–∞—é—Ç—Å—è –≤—Å–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å:
# —Ç–∏–ø —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è, —á–∞—Å—Ç—å —Ä–µ—á–∏, –∫–æ–ª-–≤–æ —Å–ª–æ–≤ –∏–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
# –î–æ–±–∞–≤–∏—Ç—å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —É—á–∏—Ç–µ–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –ø–æ—Å–ª–µ –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏ –≤—ã–≥—Ä—É–∑–∏—Ç—å –≤ —Ñ–∞–π–ª. –ù–∞ –±–ª–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–±–∞–≤–∏—Ç—å –æ–ø—Ü–∏—é
# "–ó–∞–≥—Ä—É–∑–∏—Ç—å –≥–æ—Ç–æ–≤—ã–π —É—Ä–æ–∫". –≠—Ç–æ —Ñ–∏—á–∞, —á—Ç–æ–±—ã —É—á–∏—Ç–µ–ª—å –º–æ–≥ —É —Å–µ–±—è —Å–æ—Å—Ç–∞–≤–∏—Ç—å —É—Ä–æ–∫, –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –µ–≥–æ —É—á–µ–Ω–∏–∫—É, –∞ –æ–Ω —Å–º–æ–≥ –±—ã –µ–≥–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —É —Å–µ–±—è